{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Developing an Information Retrieval System with Document Ranking\n",
    "\n",
    "This project aims to augment the Information Retrieval (IR) system developed in the previous assignments by incorporating different Document Ranking strategies. You should use the Cranfield collection as the dataset. You can find\n",
    "that in the original format here or in the TREC XML format with binary tagging here.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Overview\n",
    "\n",
    "In this project, you will implement three different approaches for document ranking, including the vector space model,\n",
    "the binary independence model, and the language model. Then, you need to compare these ranking models resorting\n",
    "to the evaluation criteria in Lecture 7. Key components and functionalities are as follows:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ⬜ Document Preprocessing\n",
    "\n",
    "Your project will begin by reading and preprocessing a collection of text documents\n",
    "– for each document you only need to retain the text with TITLE and TEXT tags. The dataset also contains\n",
    "queries and relevant documents to each query which will be useful in the evaluation phase.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path  # For standard paths that work on both windows and linux\n",
    "import pickle  # For write/read dicts to/from files\n",
    "import xml.etree.ElementTree as ET # For reading xml files\n",
    "from nltk.tokenize import RegexpTokenizer  # For tokenization\n",
    "from nltk.stem import PorterStemmer  # For stemming\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path(\"cranfield-dataset\") / \"cran.all.1400.xml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== DOC 0 ========\n",
      "experimental investigation of the aerodynamics of a\n",
      "wing in a slipstream .\n",
      "======== DOC 1 ========\n",
      "simple shear flow past a flat plate in an incompressible fluid of small\n",
      "viscosity .\n"
     ]
    }
   ],
   "source": [
    "root = ET.parse(DATA_PATH).getroot()\n",
    "\n",
    "for i, doc in enumerate(root.findall('doc')):\n",
    "    print(f\"======== DOC {i} ========\")\n",
    "    print(doc.find('title').text)\n",
    "    if i >= 1 : break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = []\n",
    "posting_list = {}\n",
    "\n",
    "for doc_id, doc in enumerate(root.findall(\"doc\")):\n",
    "    text = doc.find(\"text\").text or \"\"\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [ps.stem(token) for token in tokens]\n",
    "\n",
    "    docs.append(text)\n",
    "\n",
    "    for term in tokens:\n",
    "        if posting_list.get(term) == None:\n",
    "            posting_list[term] = {}\n",
    "        \n",
    "        if posting_list[term].get(doc_id) == None:\n",
    "            posting_list[term][doc_id] = 0\n",
    "\n",
    "        posting_list[term][doc_id] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "but(219 docs): [doc_id: frequency] {13: 2, 24: 1, 27: 1, 43: 1, 48: 2, 61: 1, 65: 1, 71: 2, 93: 1, 109: 2, 113: 1, 115: 1, 124: 1, 125: 1, 131: 1, 137: 1, 139: 1, 148: 1, 151: 1, 152: 1, 155: 2, 159: 1, 167: 1, 172: 1, 175: 1, 178: 1, 184: 1, 187: 1, 188: 1, 190: 1, 192: 1, 198: 2, 200: 1, 201: 4, 203: 1, 205: 1, 208: 1, 209: 1, 211: 2, 213: 1, 220: 1, 226: 1, 228: 1, 240: 1, 243: 1, 246: 1, 251: 2, 254: 2, 260: 1, 265: 1, 271: 1, 282: 1, 283: 1, 291: 1, 295: 1, 328: 4, 337: 2, 346: 1, 347: 1, 351: 1, 369: 1, 374: 1, 389: 1, 403: 2, 416: 1, 430: 1, 440: 1, 451: 1, 458: 2, 476: 2, 483: 1, 489: 1, 498: 2, 510: 1, 514: 1, 518: 1, 519: 1, 520: 1, 521: 1, 526: 1, 535: 1, 544: 1, 546: 1, 555: 1, 561: 1, 563: 1, 565: 1, 566: 1, 568: 1, 569: 1, 571: 1, 588: 1, 594: 1, 599: 1, 603: 1, 635: 1, 639: 1, 642: 1, 651: 1, 659: 1, 660: 1, 662: 1, 666: 1, 670: 1, 672: 1, 678: 1, 684: 2, 685: 1, 691: 1, 703: 2, 717: 1, 720: 1, 726: 1, 730: 1, 738: 1, 747: 1, 751: 1, 752: 2, 755: 1, 756: 2, 759: 1, 762: 1, 785: 1, 795: 1, 796: 3, 797: 2, 798: 1, 808: 1, 809: 1, 810: 1, 813: 1, 817: 1, 819: 1, 823: 1, 841: 1, 867: 1, 868: 1, 872: 1, 873: 1, 876: 1, 882: 1, 888: 1, 891: 1, 901: 1, 902: 1, 907: 1, 908: 1, 910: 1, 912: 1, 922: 1, 927: 1, 929: 1, 932: 1, 961: 3, 965: 2, 976: 1, 983: 1, 984: 1, 989: 1, 993: 1, 999: 2, 1000: 1, 1006: 1, 1011: 1, 1034: 1, 1039: 1, 1041: 1, 1046: 2, 1050: 1, 1053: 1, 1065: 1, 1067: 1, 1074: 1, 1077: 1, 1081: 2, 1083: 1, 1109: 1, 1130: 1, 1135: 1, 1164: 1, 1174: 1, 1177: 1, 1181: 1, 1184: 1, 1187: 2, 1188: 1, 1205: 1, 1206: 1, 1209: 1, 1217: 1, 1219: 1, 1224: 1, 1234: 1, 1237: 1, 1238: 2, 1241: 2, 1243: 2, 1244: 1, 1259: 1, 1264: 1, 1270: 1, 1276: 1, 1302: 1, 1308: 1, 1315: 1, 1319: 1, 1320: 1, 1321: 1, 1324: 1, 1336: 1, 1342: 1, 1369: 1, 1371: 1, 1372: 1, 1374: 2, 1379: 1, 1383: 1, 1384: 1, 1391: 1}\n"
     ]
    }
   ],
   "source": [
    "print(f\"but({len(posting_list[\"but\"])} docs): [doc_id: frequency]\",posting_list[\"but\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of word 'but' in doc_13 = 2\n"
     ]
    }
   ],
   "source": [
    "# test if above result is correct\n",
    "num_but_in_doc_13 = sum(np.array(docs[13].split()) == \"but\")\n",
    "print(\"Number of word 'but' in doc_13 =\", num_but_in_doc_13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the posting list in a file\n",
    "with open(Path('files') / \"posting_list.pkl\", \"wb\") as f:\n",
    "    pickle.dump(posting_list, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
